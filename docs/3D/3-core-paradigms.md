---
title: Core Paradigms
description: "Understanding polygonal and NURBS modeling paradigms and subdivision surfaces."
sidebar_position: 3
---

import PolyVsNurbsInteractive from '@site/src/components/poly-vs-nurbs-interactive';

# Core 3D Data Paradigms

<div style={{display: 'flex', justifyContent: 'space-between', alignItems: 'center', flexWrap: 'wrap', marginBottom: '20px', gap: '15px'}}>
  <div style={{display: 'flex', gap: '8px', flexWrap: 'wrap'}}>
    <span style={{backgroundColor: '#4caf50', color: 'white', padding: '4px 10px', borderRadius: '12px', fontSize: '12px', fontWeight: 'bold'}}>BEGINNER</span>
  </div>
  <div style={{display: 'flex', alignItems: 'center', gap: '6px', color: '#666', fontSize: '14px', whiteSpace: 'nowrap'}}>
    <span style={{fontSize: '16px'}}>ðŸ“–</span>
    <span>6 min read</span>
  </div>
</div>

---

<div style={{
  position: 'relative',
  paddingBottom: '56.25%',
  height: 0,
  overflow: 'hidden',
  maxWidth: '100%',
  marginBottom: '30px',
  borderRadius: '8px',
  boxShadow: '0 4px 6px rgba(0,0,0,0.1)'
}}>
  <iframe
    style={{
      position: 'absolute',
      top: 0,
      left: 0,
      width: '100%',
      height: '100%',
      border: 0
    }}
    src="https://www.youtube.com/embed/CKRIjej1nMI"
    title="YouTube video player"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowFullScreen>
  </iframe>
</div>

## Introduction
Now that we've established what 3D space is from first principles, we need to understand how data populates that space. There are several distinct paradigms for representing 3D geometry â€” polygonal, NURBS, volumetrics, and point clouds â€” each built on fundamentally different mathematical principles. Each interprets 3D space differently, and moving between paradigms requires deliberate conversion rather than seamless compatibility.

Blender is fundamentally a polygonal modeling tool. This is its core strength, though other paradigms are accessible. In a visual investigative context, you're likely to encounter point clouds from LiDAR or photoscans, volumetric data from scientific sources, and traditional polygonal geometry all in the same project. Understanding these distinctions prevents treating incompatible data as if it follows the same rules.

In the following sections, we'll discuss volumetrics, point clouds, and NURBS first, before concluding with polygonal modeling â€” the paradigm at the heart of Blender's design.

## Volumetrics

Volumetric data fills 3D space with information rather than defining surfaces. Imagine dividing your 3D space into millions of tiny boxes stacked on top of each otherâ€”each box contains a value like density or distance to a surface. This is how volumetric data populates space.

Volumetrics excel at representing phenomena hard to capture with surfaces: smoke, fog, liquids, or complex shapes. They're fast for spatial calculations and handle complicated geometry well. However, they require discretization into a grid, so resolution becomes a tradeoff between accuracy and memory. Visualizing volumetrics requires conversion to another format, usually meshes.

You'll encounter volumetric data in scientific and medical contexts: CT scans, weather simulations, or specialized scanning equipment. In Blender, volumetrics can be imported and rendered but not robustly edited. Your typical workflow is: import to visualize, then convert to a mesh if you need to modify it.

## Point Clouds

A point cloud is a collection of individual points floating in 3D space. Each point has coordinates (x, y, z) and often additional data like color or intensity. That's itâ€”no surfaces, no connections.

Point clouds are generated by real-world scanners: LiDAR captures laser bounces, photogrammetry reconstructs 3D from photographs, and depth sensors produce point data. Their strength is directnessâ€”they're unprocessed measurement data representing what a scanner actually captured. This makes them valuable for visualization and analysis.

However, individual points don't define surfaces or solid shapes. Converting to a mesh is non-trivial, and point clouds have no topologyâ€”no notion of "inside" or "outside." Real scanner data is also often noisy with gaps and stray points.

In Blender, you can display point clouds but not edit them robustly. Most work involves visualization or converting to a mesh for further modeling.

## NURBS

NURBS (Non-Uniform Rational B-Splines) defines smooth shapes using control points and mathematical equations. Think of it like a flexible rubber sheet held in place by pinsâ€”move the pins and the sheet deforms smoothly.

NURBS dominates CAD and industrial design software like Rhino and SolidWorks. It's central to parametric modeling, where relationships between objects are rule-based: change one dimension and the entire model updates automatically. This makes NURBS powerful for design iteration.

NURBS excels at mathematical precision and smooth, continuous surfaces. It struggles with sharp details, complex topology, or irregular organic forms. The mathematical complexity also means operations can produce unexpected results.

You'll encounter NURBS in engineering, architecture, or product design files. Blender supports NURBS but it's not where its strength lies. Most workflows convert NURBS to polygonal meshes for work in Blender.

:::caution
Even though NURBS is primarily used in engineering and product development, it is a common misconception that Blender does not allow to be used for similar applications. It is possible to create highly accurate models in Blender that can be used for 3D printing and similar precision-requiring workflows.
:::

## Polygonal vs. NURBS: The Modeling Decision

When building 3D geometry from scratch, the practical choice narrows to two paradigms: NURBS and polygonal modeling. Let's have a look at how they compare in their essence.

<PolyVsNurbsInteractive />

If we play around with the element above, a critical tradeoff emerges. NURBS surfaces achieve smooth curves with significantly fewer control points than polygonal modeling requires. Polygons approximate smoothness through subdivisionâ€”a recursive technique dividing faces to create finer geometryâ€”but this increases face count exponentially, making the mesh computationally heavier. This explains why NURBS excels at smooth, precision-defined surfaces, while polygonal subdivision offers you direct control over mesh density and structure.

### The "Smoothness" Solution: Subdivision Surfaces (SubD)

Subdivision Surface modeling bridges the gap between raw polygonal modeling and the smooth curves NURBS provide. You model a low-resolution "cage" (simple polygons), and Blender calculates a mathematically smooth surface based on that cage.

:::info Constraint
You must maintain "all-quad" or "mostly-quad" topology (four-sided faces) to ensure the subdivision algorithm works without artifacts. More on that in the anatomy of a 3D model chapter.
:::

## Conclusion
Blender is built for the polygonal paradigm, and that will be our focus. But this doesn't mean we cannot include data from other paradigmsâ€”we can import, visualize, and convert between them as needed.